<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis">
    <meta name="author" content="Eric R. Chan,
                                 Marco Monteiro,
                                 Other Authors">

    <title>pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->

    <!-- Loads <model-viewer> for modern browsers: -->
    <script type="module"
            src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
    </script>
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>pi-GAN: Periodic Implicit Generative Adversarial Networks</h2>
    <h2>for 3D-Aware Image Synthesis</h2>
    <hr>
    <p class="authors">
        <a href="http://www.jmartel.net"> Eric R. Chan*</a>,
        <a href="">Marco Monteiro*</a>,</br>
        <a href="https://stanford.edu/~gordonwz/"> Gordon Wetzstein</a>
        <a href="">Other Authors</a>,</br>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary"
           href="">Paper</a>
        <a class="btn btn-primary disabled" href="">Code</a>
        <a class="btn btn-primary disabled" href="">Data</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/pi-GAN.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <hr>
        <p>
            We have witnessed rapid progress on 3D-aware image synthesis,
            leveraging recent advances in generative visual models and neural
            rendering. Existing approaches however fall short in two ways:
            first, they may lack an underlying 3D representation or rely on
            view-inconsistent rendering, hence synthesizing images that are
            not multi-view consistent; second, they often depend upon
            representation network architectures that are not expressive
            enough, and their results thus lack in image quality. We propose
            a novel generative model, named Periodic Implicit Generative
            Adversarial Networks (\model or pi-GAN), for high-quality 3D-aware
            image synthesis. \model leverages neural representations with
            periodic activation functions and volumetric rendering to represent
            scenes as view-consistent 3D representations with fine detail.
            The proposed approach obtains state-of-the-art results for 3D-aware
            image synthesis with multiple real and synthetic datasets.
        </p>
    </div>

    <div class="section">
        <h2>Results</h2>
        <hr>
        <p>
            We have witnessed rapid progress on 3D-aware image synthesis,
            leveraging recent advances in generative visual models and neural
            rendering.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="vids/results.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Interpreting the 3D Representation</h2>
        <hr>
        <p>
            pi-GAN relies on an underlying multi-view-consistent 3D representation. While the color output of pi-GAN at a given position depends on ray
            direction, the density output is completely view independent. An
            interpretable 3D represenation can be extracted and visualized using
            marching cubes[cite] on the density output of the conditioned radiance
            field to produce a surface mesh.

        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="80%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/asdf.mp4" type="video/mp4">
                </video>
            </div>
        </div>

        <div class="container">
                    <div class="row align-items-center">
                        <div class="col-md-6 padding-0 canvas-row">
                            <h4>CelebA</h4>
                            <model-viewer
                                    alt="Ground Truth"
                                    src="glbs/face.glb"
                                    style="width: 100%; height: 200px; background-color: #404040"
                                    exposure=".8"
                                    camera-orbit="0deg 75deg 105%"
                                    auto-rotate
                                    camera-controls>
                            </model-viewer>
                        </div>
                        <div class="col-md-6 padding-0 canvas-row">
                            <h4>CARLA</h4>
                            <model-viewer
                                    alt="DeepSDF"
                                    src="glbs/car.glb"
                                    style="width: 100%; height: 200px; background-color: #404040"
                                    exposure=".8"
                                    auto-rotate
                                    camera-controls>
                            </model-viewer>
                            <!--                            poster="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7e6db2d75a9b467eee4111_legomesh_cover.png"-->
                        </div>
                    </div>
          </div>

    </div>

    <div class="section">
        <h2>Inverse Rendering and Novel View Synthesis</h2>
        <hr>
        <p>
            Using a trained pi-GAN generator we can perform single-view reconstruction
            and novel-view synthesis using the procedure described by Karras et al.~\cite{Karras2020stylegan2}.
            We freeze the parameters for our implicit representaion and seek the frequencies and
            phase shifts for each MLP layer which produce a radiance field that, when rendered,
            best matches the target image.
        </p>
        <div class="container">
            <div style="text-align:center">
                <div style="width:65%; margin:auto">
                <div class="row align-items-center" >
                    <!-- <img src="vids/inverse_render_gt.jpg" width="40%">
                    <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                        <source src="vids/inverse_render.mp4" type="video/mp4">
                    </video> -->
                    <div class="col-md-6 padding-0 canvas-row">
                        <h4>Target</h4>
                        <div class="row align-items-center">
                            <div class="col justify-content-center text-center">
                                <img src="vids/inverse_render_gt.jpg" width="100%">
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6 padding-0 canvas-row">
                        <h4>Reconstruction</h4>
                        <div class="row align-items-center">
                            <div class="col justify-content-center text-center">
                                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                                    <source src="vids/inverse_render.mp4" type="video/mp4">
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
                </div>
            </div>
        </div>
        
    </div>

    <div class="section">
        <h2>pi-GAN Extrapolates to Unseen Camera Poses</h2>
        <hr>
        <p>
            Because pi-GAN relies on an underlying 3D structural representaion and offers
            explicit camera control, it more readily renders views absent
            from the training distribution of camera poses, than previous methods
            that rely on black-box represenations or projections.
            pi-GAN was only trained on tightly cropped images, but the radiance field
            extrapolates when we zoom out the camera.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="30%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="vids/extrapolation.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </div>








    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://papers.nips.cc/paper/8396-scene-representation-networks-continuous-3d-structure-aware-neural-scene-representations"
                   class="list-group-item">
                    <img src="img/paper_thumbnails.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{sitzmann2019metasdf,
                author = {Sitzmann, Vincent
                          and Chan, Eric R.
                          and Tucker, Richard
                          and Snavely, Noah
                          and Wetzstein, Gordon},
                title = {MetaSDF: Meta-Learning Signed
                         Distance Functions},
                booktitle = {arXiv},
                year={2020}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="http://web.stanford.edu/~sitzmann/">Vincent Sitzmann</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
